---
title: "TD 3 de Statistique descriptive 2"
author: "Miguel PALENCIA-OLIVAR"
date: '2022-02-14'
output: 
  html_document:
    keep_md: true
    df_print: paged
---
# Préambule

Le présent document a pour objectif de présenter synthétiquement - et au fur et
à mesure des séances - la solution du TD 3 correspondant au cours de Statistique
descriptive 2 dispensé en L2 MIASHS à l'Université Lumière Lyon 2 par [Stéphane CHRÉTIEN](https://sites.google.com/site/stephanegchretien/enseignement/l2-miashs-statistiques-descriptives/l2-statistiques-descriptives-2-regression-et-classification). Les consignes et le corrigé sont trouvables dans le répertoire `doc` ; `data` contient pour sa part les jeux de données utilisés dans le cadre du TD dans des formats simples d'usage.

*Ce document n'est pas un tutoriel pour R, et n'a pas pour but de remplacer le CM. En revanche, c'est la suite directe du TD 2, [trouvable ici](https://github.com/mpalenciaolivar/L2MIASHS2022-StatDesc2-TD2)*.

# Ressources utiles
## Ouvrages
- [Cours de Ricco RAKOTOMALALA sur la régression logistique (davantage de Statistique)](http://eric.univ-lyon2.fr/~ricco/cours/cours_regression_logistique.html)


## Ressources internet
- [Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)

- [LOGIT REGRESSION | R DATA ANALYSIS EXAMPLES](https://stats.oarc.ucla.edu/r/dae/logit-regression/))

# Exercice
Ce TD est un peu différent ; il s'agit de vous mettre un pied à l'étrier sur la pratique de la classification. Cet exercice est inédit, en ce sens qu'il n'est pas compris dans le sujet de TD. À ce titre, on mettra l'accent sur les ressources suivantes et sur le langage R :
- [Introduction au Data Mining](https://eric.univ-lyon2.fr/~ricco/cours/slides/Introduction_au_Data_Mining.pdf)

- [Introduction à la data science – Du data mining au big data analytics](https://eric.univ-lyon2.fr/~ricco/cours/slides/intro_ds_from_dm_to_bd.pdf)

- [Apprentissage supervisé](https://eric.univ-lyon2.fr/~ricco/cours/slides/Apprentissage_Supervise.pdf)

- [Régression logistique façon Machine Learning](http://eric.univ-lyon2.fr/~ricco/cours/slides/logistic_regression_ml.pdf)

- [Classifieur bayésien naïf](http://eric.univ-lyon2.fr/~ricco/cours/slides/naive_bayes_classifier.pdf)

- [Tutoriel Titanic (repris partiellement)](https://medium.com/analytics-vidhya/a-beginners-guide-to-learning-r-with-the-titanic-dataset-a630bc5495a8)


**Il ne sert à rien de courir : il faut d'abord lire les ressources (tutoriel exclus) PUIS faire le TD, et pas l'inverse.**

## Contexte
On cherche à prédire quels sont les individus qui survivront au nauffrage du [Titanic](https://fr.wikipedia.org/wiki/Titanic) à partir d'un certain nombre de variables.

## Dictionnaire de données
![](img/datadict.png)

Avant même de considérer le code, il va falloir faire un petit travail de qualification des données. Ce n'est pas parce qu'une variable prend des valeurs "chiffre" que c'est une variable quantitative continue (ex : un rang).

## Pré-requis
Le projet va nécessiter un certain nombre de packages externes. Pour les installer, on ouvrira `requirements.R` et on cliquera sur `Run All` (CTRL+ALT+R).

## Chargement des données
```{r}
# na.string est initialisé de la sorte afin que l'on puisse mieux gérer les données manquantes par la suite.
titanic <- read.csv(file.path("data", "titanic", "train.csv"), na.strings = "")
test <- read.csv(file.path("data", "titanic", "test.csv"), na.strings = "")
```

## Préparation des données
Comme souvent lorsque l'on travaille avec des données, il y a des valeurs manquantes, des colonnes mal typées, etc. Il faut y remédier avant toute chose. Commençons par jeter un oeil à nos données chargées :
```{r}
# Attention : cette fonction est faite pour fonctionner avec RStudio. Elle est utile pour faire des tris "à la Excel". Elle est commentée pour pouvoir exécuter le notebook sans interruption.
# View(titanic)
```

Voyons voir s'il y a des données manquantes :
```{r}
library(Amelia)  # Chargement de lib
missmap(titanic, col = c("black", "grey"))
```

Nous avons une variable Cabin qui a beaucoup de données manquantes, et une variable PassengerId qui est une pseudo-variable d'identifiants. Nous les excluerons de l'analyse. Nous suivrons notre tutoriel de référence dans un souci de simplicité (pour la pédagogie, donc). Mais la décision de faire d'abandonner d'autres variables que celles mentionnées précédemment qui a été prise par l'auteur du tuto me semble peu justifiée à certains égards.

```{r}
library(dplyr)

titanic <- select(titanic, Survived, Pclass, Age, Sex, SibSp, Parch)
test <- select(test, Survived, Pclass, Age, Sex, SibSp, Parch)
```

Il ne restera plus qu'à retirer les valeurs manquantes pour savoir ce que l'on traitera.

```{r}
titanic <- na.omit(titanic)
test <- na.omit(test)
```

Il s'agit maintenant de savoir comment sont codées nos variables.

```{r}
str(titanic)
```

Survived et Pclass sont considérées comme étant des variables numériques. Or, elles sont respectivement catégorielle et catégorielle *ordinale*. Transformons les.

```{r}
titanic$Survived <- factor(titanic$Survived)
titanic$Pclass <- factor(titanic$Pclass, order=TRUE, levels = c(3, 2, 1))

test$Survived <- factor(test$Survived)
test$Pclass <- factor(test$Pclass, order=TRUE, levels = c(3, 2, 1))
```

## Exploration des données
Il est temps de commencer à regarder nos données. Allons-y !

### Corrélations
```{r}
library(GGally)

ggcorr(titanic,
       nbreaks = 6,
       label = TRUE,
       label_size = 3,
       color = "grey50")
```

### Comptage du nombre de survivants
```{r}
library(ggplot2)

ggplot(titanic, aes(x = Survived)) +
  geom_bar(width=0.5, fill = "coral") +
  geom_text(stat='count', aes(label=stat(count)), vjust=-0.5) +
  theme_classic()
```

### Comptage du nombre de survivants en fonction du sexe
```{r}
ggplot(titanic, aes(x = Survived, fill = Sex)) +
 geom_bar(position = position_dodge()) +
 geom_text(stat='count', 
           aes(label=stat(count)), 
           position = position_dodge(width=1), vjust=-0.5)+
 theme_classic()
```

### Comptage du nombre de survivants en fonction de Pclass
```{r}
ggplot(titanic, aes(x = Survived, fill = Pclass)) +
 geom_bar(position = position_dodge()) +
 geom_text(stat='count',
           aes(label=stat(count)), 
           position = position_dodge(width=1), 
           vjust=-0.5)+
 theme_classic()
```

### Densité de l'âge
```{r}
ggplot(titanic, aes(x = Age)) +
 geom_density(fill='coral')
```

### Survie en fonction de l'âge
```{r}
# Discrétisation de l'âge
titanic$Discretized.age <- cut(titanic$Age, c(0, 10, 20, 30, 40 ,50, 60, 70, 80, 100))

ggplot(titanic, aes(x = Discretized.age, fill = Survived)) +
  geom_bar(position = position_dodge()) +
  geom_text(stat='count', aes(label=stat(count)), position = position_dodge(width=1), vjust=-0.5)+
  theme_classic()

titanic$Discretized.age <- NULL
```

## Constitution des jeux pour l'apprentissage
```{r}
# Attention: il n'y a pas de randomisation ici puisque le jeu de données est
# déjà randomisé.
# S'il y en avait, il aurait fallu exécuter la fonction avec une seed, tel que :

# set.seed(2022)

train_validation_split <- function(data, fraction = 0.8, train = TRUE) {
  total_rows <- nrow(data)
  train_rows <- fraction * total_rows
  sample <- 1:train_rows
  if (train == TRUE) {
    return (data[sample, ])
  } else {
    return (data[-sample, ])
  }
}


train <- train_validation_split(titanic, 0.8, train = TRUE)
validation <- train_validation_split(titanic, 0.8, train = FALSE)
```

## Arbre de décision
```{r}
library(rpart)
library(rpart.plot)

set.seed(2022)
dtree <- rpart(Survived ~ ., data = train, method = 'class')
rpart.plot(dtree, extra = 106)
```

On évalue le modèle:

```{r}
library(MLmetrics)

y_pred <- predict(dtree, validation, type = 'class')
y_true <- validation$Survived

dtree_precision <- Precision(y_true, y_pred, positive = 1)
dtree_recall <- Recall(y_true, y_pred, positive = 1)
dtree_f1 <- F1_Score(y_true, y_pred, positive = 1)
dtree_auc <- AUC(y_true, y_pred)

paste0("Precision: ", dtree_precision)
paste0("Recall: ", dtree_recall)
paste0("F1 Score: ", dtree_f1)
paste0("AUC: ", dtree_auc)
```


```{r}
library(ROSE)

roc.curve(y_pred, y_true, plotit = TRUE, add.roc = FALSE, 
          n.thresholds=100)
```

On affine notre arbre (fine tuning) :

```{r}
set.seed(2022)

control <- rpart.control(minsplit = 8,
                         minbucket = 2,
                         maxdepth = 6,
                         cp = 0)
dtree_tuned_fit <- rpart(Survived ~ ., data = train, method = 'class', control = control)
y_pred <- predict(dtree_tuned_fit, validation, type = 'class')

dtree_tuned_fit_precision <- Precision(y_true, y_pred, positive = 1)
dtree_tuned_fit_recall <- Recall(y_true, y_pred, positive = 1)
dtree_tuned_fit_f1 <- F1_Score(y_true, y_pred, positive = 1)
dtree_tuned_fit_auc <- AUC(y_true, y_pred)

paste0("Precision: ", dtree_tuned_fit_precision)
paste0("Recall: ", dtree_tuned_fit_recall)
paste0("F1 Score: ", dtree_tuned_fit_f1)
paste0("AUC: ", dtree_tuned_fit_auc)
```
```{r}
rpart.plot(dtree_tuned_fit, extra = 106)
```


```{r}
roc.curve(y_pred, y_true, plotit = TRUE, add.roc = FALSE, 
          n.thresholds=100)
```

## Régression logistique
Même si la régression logistique est ici présentée sous l'angle de la classification, elle est fondamentalement, intrinsèquement, décidément - notez que j'insiste - une régression. On renverra surtout aux ressources mises en ligne par R. RAKOTOMALALA pour la vision Statistique de la chose.

```{r}
set.seed(2022)

# On standardise les variables numériques
data_rescale <- mutate_if(titanic,
                          is.numeric,
                          list(~as.numeric(scale(.))))
train <- train_validation_split(data_rescale, 0.7, train = TRUE)
validation <- train_validation_split(data_rescale, 0.7, train = FALSE)
logreg <- glm(Survived ~ ., data = train, family = "binomial")
summary(logreg)
```

Cette sortie est la même que pour une régression linéaire. Encore une fois, c'est parce que la *régression logistique est une régression et non un classifieur en soi*. Voyons comment cela se passe sur la significativité globale :

```{r}
LR <- 677.21 - 461.88
p <- 498 - 492
pchisq(LR, p, lower.tail = F)
```

En supposant un risque à 5%, on est bien en-deçà. On a donc bien un modèle informatif.

```{r}
y_true <- validation$Survived
y_pred <- predict(logreg, validation, type = 'response')
y_pred <- as.factor(ifelse(y_pred > 0.5, 1, 0))

logreg_precision <- Precision(y_true, y_pred, positive = 1)
logreg_recall <- Recall(y_true, y_pred, positive = 1)
logreg_f1 <- F1_Score(y_true, y_pred, positive = 1)
logreg_auc <- AUC(y_true, y_pred)
paste0("Precision: ", logreg_precision)
paste0("Recall: ", logreg_recall)
paste0("F1 Score: ", logreg_f1)
paste0("AUC: ", logreg_auc)
```

```{r}
roc.curve(y_pred, y_true, plotit = TRUE, add.roc = FALSE, 
          n.thresholds=100)
```

## Classifieur bayésien naïf
```{r}
library(e1071)

set.seed(2022)
nbClassifier <- naiveBayes(Survived ~., data = train)
y_pred <- predict(nbClassifier, validation)
y_true <- validation$Survived

nbClassifier_precision <- Precision(y_true, y_pred, positive = 1)
nbClassifier_recall <- Recall(y_true, y_pred, positive = 1)
nbClassifier_f1 <- F1_Score(y_true, y_pred, positive = 1)
nbClassifier_auc <- AUC(y_true, y_pred)
paste0("Precision: ", nbClassifier_precision)
paste0("Recall: ", nbClassifier_recall)
paste0("F1 Score: ", F1_Score(y_true, y_pred, positive = 1))
paste0("AUC: ", nbClassifier_auc)
```

```{r}
roc.curve(y_pred, y_true, plotit = TRUE, add.roc = FALSE, 
          n.thresholds=100)
```

## Comparaison et sauvegarde d'un modèle
Nous avons 4 modèles. L'heure est à la comparaison des performances. Mon indice
préféré est le F1-Score, car il s'agit d'une moyenne (harmonique). En fonction
des cas, on pourra placer davantage d'importance sur la précision ou sur le
rappel, et tout de même synthétiser cela au sein du Beta-F1-Score. Faisons un
dataframe de nos indices.

```{r}
dtree_perfs <- list(precision = dtree_precision,
                    recall = dtree_recall,
                    f1_score = dtree_f1,
                    auc = dtree_auc)

dtree_tuned_fit_perfs <- list(precision = dtree_tuned_fit_precision,
                              recall = dtree_tuned_fit_recall,
                              f1_score = dtree_tuned_fit_f1,
                              auc = dtree_tuned_fit_auc)

logreg_perfs <- list(precision = logreg_precision,
                     recall = logreg_recall,
                     f1_score = logreg_f1,
                     auc = logreg_auc)

nbClassifier_perfs <- list(precision = nbClassifier_precision,
                           recall = nbClassifier_recall,
                           f1_score = nbClassifier_f1,
                           auc = nbClassifier_auc)

# Pas très élégant, mais ça marche
perfs <- as.data.frame(t(do.call(rbind, Map(data.frame,
                       dtree = dtree_perfs,
                       dtree_tuned = dtree_tuned_fit_perfs,
                       logreg = logreg_perfs,
                       nbClassifier = nbClassifier_perfs))))

# On trie par f1-score décroissant
attach(perfs)
perfs[order(-f1_score),]
detach(perfs)
```

Notre gagnant est l'arbre de décision avec tuning. Puisque nous avons notre
vainqueur, il peut être judicieux de le sauvegarder pour le réutiliser. C'est
très facile à faire (décommenter les fonctions):

```{r}
# saveRDS(dtree_tuned_fit, "dtree_tuned_fit.rds")
# dtree_tuned_fit <- readRDS("dtree_tuned_fit.rds")
```

## Test du modèle
Nous avons entraîné/tuné un modèle ; il est maintenant temps de le tester sur
des données totalement inédites du point de vue de l'entraînement. Il faut
traiter les données nouvelles de la même façon que les données d'entraînement
au préalable. Cela a été fait en même temps que le reste des données, dans les
sections dédiées. Voyons maintenant la performance "réelle", sur données test:

```{r}
y_pred <- predict(dtree_tuned_fit, test, type = 'class')
y_true <- test$Survived

dtree_tuned_fit_precision <- Precision(y_true, y_pred, positive = 1)
dtree_tuned_fit_recall <- Recall(y_true, y_pred, positive = 1)
dtree_tuned_fit_f1 <- F1_Score(y_true, y_pred, positive = 1)
dtree_tuned_fit_auc <- AUC(y_true, y_pred)

paste0("Precision: ", dtree_tuned_fit_precision)
paste0("Recall: ", dtree_tuned_fit_recall)
paste0("F1 Score: ", dtree_tuned_fit_f1)
paste0("AUC: ", dtree_tuned_fit_auc)
```
